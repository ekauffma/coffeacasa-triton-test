{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cd74d65-2b7e-48b7-a7ec-cd0ba4fc748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import uproot\n",
    "import hist\n",
    "import awkward as ak\n",
    "\n",
    "import tritonclient.grpc as grpcclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "\n",
    "from coffea import processor\n",
    "from coffea.nanoevents.schemas.base import BaseSchema, zip_forms\n",
    "from coffea.nanoevents.methods import base\n",
    "\n",
    "if sys.version_info >= (3, 0):\n",
    "    import queue\n",
    "else:\n",
    "    import Queue as queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a78a21d-daf6-4884-afca-9c6c65d36e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "\n",
    "verbose     = False # enable verbose output for grpcclient\n",
    "batch_size  = 1000 # number of events to send in one inference request\n",
    "url         = \"agc-triton-inference-server:8001\" # url of inference server. use 8001 for grpcclient and 8000 for httpclient\n",
    "test_events = \"testevents.csv\" # input csv file\n",
    "model_name  = \"binary_classifier\" # name of ML model to use (make sure it is loaded properly)\n",
    "model_vers  = \"\" # specify model version if necessary\n",
    "num_batches = 5 # number of batches to process (number of events will be num_batches*batch_size)\n",
    "num_cores   = 4 # scaling for setup with FuturesExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d3df3e-899d-4387-9fec-bbb24e5e7f2e",
   "metadata": {},
   "source": [
    "## Set up gRPC client and get model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e926dd23-1779-4013-8a38-cce6b534f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create gRPC client (communicates with inference server)\n",
    "triton_client = grpcclient.InferenceServerClient(url=url, \n",
    "                                                 verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca236889-518e-4677-a87f-8438ceea1e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata = triton_client.get_model_metadata(model_name, model_vers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ff913fc-f12d-476d-86df-c6ef5628e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = triton_client.get_model_config(model_name=model_name, \n",
    "                                              model_version=model_vers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9bc2794-bf4d-4461-8083-6fd15786e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = model_config.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "217cb2c2-7676-44a0-816d-72e25bf08c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_metadata = model_metadata.inputs[0]\n",
    "input_config = model_config.input[0]\n",
    "output_metadata = model_metadata.outputs[0]\n",
    "\n",
    "input_batch_dim = (model_config.max_batch_size > 0)\n",
    "\n",
    "max_batch_size = model_config.max_batch_size\n",
    "input_name = input_metadata.name\n",
    "output_name = output_metadata.name\n",
    "n_features = input_metadata.shape[1 if input_batch_dim else 0]\n",
    "format = input_config.format\n",
    "dtype = input_metadata.datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a8cd85-9524-4031-aa8e-b584666cdbc0",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "744c9564-9157-4fcb-ac90-474f570a7b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_base = processor.ProcessorABC\n",
    "\n",
    "class SigBkgInference(processor_base):\n",
    "    \n",
    "    def __init__(self, batch_size, model_name, model_vers, url, verbose):\n",
    "        self.hist = (hist.Hist.new.Reg(50, -7, 7, name=\"var0\", label=\"Variable 0\")\n",
    "                     .Reg(50, -45, 5, name=\"var1\", label=\"Variable 1\")\n",
    "                     .Reg(50, 0, 15, name=\"var2\", label=\"Variable 2\")\n",
    "                     .IntCat(range(2), name=\"classification\", label=\"ML Classification (Sig/Bkg)\")\n",
    "                     .Weight())\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.input_name = input_name\n",
    "        self.dtype = dtype\n",
    "        self.output_name = output_name\n",
    "        self.model_name = model_name\n",
    "        self.model_vers = model_vers\n",
    "        self.url = url\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def process(self, events):\n",
    "        \n",
    "        histogram = self.hist.copy()\n",
    "        \n",
    "        # get event variables\n",
    "        var0 = events.var0\n",
    "        var1 = events.var1\n",
    "        var2 = events.var2\n",
    "        var3 = events.var3\n",
    "        \n",
    "        features = ak.concatenate([var0[..., np.newaxis],\n",
    "                                   var1[..., np.newaxis],\n",
    "                                   var2[..., np.newaxis],\n",
    "                                   var3[..., np.newaxis]], axis=1)\n",
    "        \n",
    "        \n",
    "        # start triton client\n",
    "        triton_client = grpcclient.InferenceServerClient(url=self.url, verbose=self.verbose)\n",
    "        # model_metadata = triton_client.get_model_metadata(self.model_name, self.model_vers)\n",
    "        # model_config = triton_client.get_model_config(model_name=self.model_name, \n",
    "        #                                               model_version=self.model_vers)\n",
    "        triton_client.close()\n",
    "#         model_config = model_config.config\n",
    "#         input_metadata = model_metadata.inputs[0]\n",
    "#         input_config = model_config.input[0]\n",
    "#         output_metadata = model_metadata.outputs[0]\n",
    "        \n",
    "#         max_batch_size = model_config.max_batch_size\n",
    "#         batch_size = np.minimum(max_batch_size, self.batch_size)\n",
    "#         input_name = input_metadata.name\n",
    "#         output_name = output_metadata.name\n",
    "#         dtype = input_metadata.datatype\n",
    "        \n",
    "        # get number of batches\n",
    "        data_length = len(var0)\n",
    "        num_batches = int(np.ceil(data_length/self.batch_size))\n",
    "        \n",
    "        # make inference request for each batch\n",
    "        classification = np.zeros(data_length)\n",
    "        startind = 0\n",
    "        for i in range(num_batches):\n",
    "            \n",
    "            if i == num_batches-1: # if we are on last batch\n",
    "                data_current = features[startind:].to_numpy().astype(np.float32)\n",
    "            else:\n",
    "                data_current = features[startind:startind + self.batch_size].to_numpy().astype(np.float32)\n",
    "            startind += self.batch_size\n",
    "            \n",
    "            client = grpcclient\n",
    "            \n",
    "#             inpt = [client.InferInput(self.input_name, data_current.shape, self.dtype)]\n",
    "#             inpt[0].set_data_from_numpy(data_current)\n",
    "            \n",
    "#             output = client.InferRequestedOutput(output_name)\n",
    "            \n",
    "            \n",
    "            # get triton client\n",
    "            # results = triton_client.infer(model_name=self.model_name, \n",
    "            #                               inputs=inpt, \n",
    "            #                               outputs=[output]).as_numpy(self.output_name)\n",
    "            \n",
    "            if i == num_batches-1: # if we are on last batch\n",
    "                classification[startind:] = np.ones_like(classification[startind:])#results.T[0]\n",
    "            else:\n",
    "                classification[startind:startind + self.batch_size] = np.ones_like(classification[startind:startind + self.batch_size])#results.T[0]\n",
    "                \n",
    "        \n",
    "        histogram.fill(var0=var0, \n",
    "                       var1=var1, \n",
    "                       var2=var2, \n",
    "                       classification=classification)\n",
    "            \n",
    "        output = {\"nevents\": {events.metadata[\"dataset\"]: len(var0)}, \n",
    "                  \"hist\": histogram}\n",
    "\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bc83ee1-d699-43d8-b283-a0b6ee78ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define schema to load toy data\n",
    "\n",
    "class ToySchema(BaseSchema):\n",
    "    \n",
    "    def __init__(self, base_form):\n",
    "        super().__init__(base_form)\n",
    "        self._form[\"contents\"] = self._build_collections(self._form[\"contents\"])\n",
    "        \n",
    "    def _build_collections(self, branch_forms):\n",
    "        names = [\"var0\", \"var1\", \"var2\", \"var3\"]\n",
    "        \n",
    "        output = {}\n",
    "        for name in names:\n",
    "            output[name] = branch_forms[name]\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def behavior(self):\n",
    "        behavior = {}\n",
    "        behavior.update(base.behavior)\n",
    "        return behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4ef92e8-02ac-4173-9d31-ebc6a32082eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset = {'process0': {'files': ['data/testdata0.root',\n",
    "                                  'data/testdata1.root',\n",
    "                                  'data/testdata2.root',\n",
    "                                  'data/testdata3.root',\n",
    "                                  'data/testdata4.root'],\n",
    "                        'nevts': 5*20000}\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0df5d82d-1f7e-4d1d-a379-8e5d40998be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = processor.FuturesExecutor(workers=num_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b212c1d-c2be-4aa4-afc6-947de6dbc6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = processor.Runner(executor=executor, \n",
    "                       schema=ToySchema, \n",
    "                       savemetrics=True, \n",
    "                       metadata_cache={}, \n",
    "                       chunksize=500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a12333aa-427b-40bd-a1f5-7dd3e0074e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e002c13e50443c84e73b3740ce5d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filemeta = run.preprocess(fileset, treename=\"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e0bb224-6732-46fa-8144-1a576ed1f673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb8ce343a8842edb70985498dbb2d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_histograms, metrics = run(fileset, \n",
    "                              \"events\", \n",
    "                              processor_instance=SigBkgInference(batch_size, \n",
    "                                                                 model_name, \n",
    "                                                                 model_vers, \n",
    "                                                                 url, \n",
    "                                                                 verbose)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220b6869-49be-448d-a347-dbfa0d8786e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bffd69c-89df-41d1-95fd-2087f8244268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b7f06-6fb1-4189-930b-22c35927d106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7f729-c9d7-43c4-a571-576e2e08f5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10330bc-6906-4d52-ade8-60b5e32e4857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47424ac9-06e9-496e-b74d-c4f832fa95bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf1fb1-acae-400a-9948-8e4fbefd08e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "276875d6-e80d-4a45-9ad5-61c3dfef3638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "data = np.loadtxt(test_events, dtype=np.float32, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "50cc69f1-a97d-460a-8914-fd2e602a430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch information\n",
    "data_length = data.shape[0]\n",
    "max_num_batches = int(np.ceil(data_length/batch_size)) # maximum number of batches given number of events in data and batch_size\n",
    "    \n",
    "num_batches = np.minimum(num_batches, max_num_batches) # ensure number of batches doesn't extend beyond number of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "87402a9f-8720-4f94-a19e-0fadda39b00b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [268], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m inpt[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_data_from_numpy(data_current)\n\u001b[1;32m     16\u001b[0m output \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mInferRequestedOutput(output_name)\n\u001b[0;32m---> 18\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtriton_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                              \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                              \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m inference_output \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mas_numpy(output_name)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference Results for Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mround(inference_output)\u001b[38;5;241m.\u001b[39mT)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tritonclient/grpc/__init__.py:1421\u001b[0m, in \u001b[0;36mInferenceServerClient.infer\u001b[0;34m(self, model_name, inputs, model_version, outputs, request_id, sequence_id, sequence_start, sequence_end, priority, timeout, client_timeout, headers, compression_algorithm)\u001b[0m\n\u001b[1;32m   1418\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfer, metadata \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(metadata, request))\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1421\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_stub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModelInfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_grpc_compression_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompression_algorithm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose:\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/grpc/_channel.py:944\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    938\u001b[0m              request,\n\u001b[1;32m    939\u001b[0m              timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m              wait_for_ready\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    943\u001b[0m              compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 944\u001b[0m     state, call, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/grpc/_channel.py:933\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    926\u001b[0m     call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[1;32m    927\u001b[0m         cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[1;32m    928\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method, \u001b[38;5;28;01mNone\u001b[39;00m, _determine_deadline(deadline), metadata,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    932\u001b[0m         ),), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context)\n\u001b[0;32m--> 933\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m     _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:338\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:169\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:163\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:63\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:42\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# send inference requests    \n",
    "startind = 0\n",
    "\n",
    "for i in range(num_batches):\n",
    "        \n",
    "        data_current = data[startind:startind + batch_size, :]\n",
    "        startind += batch_size\n",
    "        \n",
    "        client = grpcclient\n",
    "        \n",
    "        print(data_current.shape)\n",
    "\n",
    "        inpt = [client.InferInput(input_name, data_current.shape, dtype)]\n",
    "        inpt[0].set_data_from_numpy(data_current)\n",
    "\n",
    "        output = client.InferRequestedOutput(output_name)\n",
    "        \n",
    "        results = triton_client.infer(model_name=model_name, \n",
    "                                      inputs=inpt, \n",
    "                                      outputs=[output])\n",
    "    \n",
    "        inference_output = results.as_numpy(output_name)\n",
    "        print(f\"Inference Results for Batch {i}: \", np.round(inference_output).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "922a56c6-39ad-4ffb-9300-359cf4cfee96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.zeros(100)\n",
    "test[80:] = np.ones(20)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a98e1-74ea-4dd5-8c6f-fb34cd5f5870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
